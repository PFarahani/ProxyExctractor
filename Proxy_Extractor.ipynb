{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92496972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import date\n",
    "import asyncio\n",
    "import aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127eb7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [page + 1 for page in range(20)]\n",
    "url = 'https://proxyhub.me/en/ir-free-proxy-list.html'\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "##########################\n",
    "async def get_proxies(session, url, page):\n",
    "    \n",
    "    tables = []\n",
    "    cookies = {\n",
    "        'page': str(page),\n",
    "        'anonymity': 'all',\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'authority': 'proxyhub.me',\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',\n",
    "        'accept-language': 'en-US,en;q=0.7',\n",
    "        'cache-control': 'max-age=0',\n",
    "        # Requests sorts cookies= alphabetically\n",
    "        'cookie': 'page='+str(page)+'; anonymity=all',\n",
    "        'referer': 'https://proxyhub.me/en/ir-free-proxy-list.html/',\n",
    "        'sec-fetch-dest': 'document',\n",
    "        'sec-fetch-mode': 'navigate',\n",
    "        'sec-fetch-site': 'same-origin',\n",
    "        'sec-fetch-user': '?1',\n",
    "        'sec-gpc': '1',\n",
    "        'upgrade-insecure-requests': '1',\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36',\n",
    "    }\n",
    "\n",
    "    async with session.get(url, cookies=cookies, headers=headers) as response:\n",
    "        response = await response.content\n",
    "        soup = BeautifulSoup(response, 'html.parser')\n",
    "        table = soup.find('table')\n",
    "        tables.append(table)\n",
    "    await asyncio.sleep(2)\n",
    "    print(\"Fetch Successful\")\n",
    "    return tables\n",
    "\n",
    "##########################\n",
    "\n",
    "async def get_tasks(session):\n",
    "    tasks = []\n",
    "    for i in range(len(pages)):\n",
    "        task = asyncio.create_task(get_proxies(session, url, pages[i]))\n",
    "        tasks.append(task)\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    print(\"Task Successful\")\n",
    "    return results\n",
    "\n",
    "\n",
    "##########################\n",
    "\n",
    "async def main():\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "\n",
    "        tables = await get_tasks(session)\n",
    "\n",
    "        return tables\n",
    "    \n",
    "\n",
    "##########################\n",
    "\n",
    "st = time.time()\n",
    "tables = await main()\n",
    "et = time.time()\n",
    "\n",
    "print(\"Elapsed Time: \", et-st)\n",
    "print(\"\\n\")\n",
    "print(\"Number of Tables: \"len(tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3a1af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = []\n",
    "\n",
    "for header in soup.findChildren('th'):\n",
    "    column_name.append(header.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb23dd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for i in range(len(a)):\n",
    "    my_table = tables[i]\n",
    "    # You can find children with multiple tags by passing a list of strings\n",
    "    rows = my_table.findChildren(['th', 'tr'])\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        cells = row.findChildren('td')\n",
    "        for cell in cells:\n",
    "            value = cell.string\n",
    "            data.append(value)\n",
    "        \n",
    "chunk_size = 6\n",
    "data = pd.DataFrame(data=[data[i:i + chunk_size] for i in range(0, len(data), chunk_size)], columns = column_name)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b003a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Country'] = data['Country'].apply(lambda x:'Iran')\n",
    "data.drop('City', axis=1,inplace=True)\n",
    "data['URL'] = dict('http://' + data['IP'].astype(str) + ':' + data['Port'].astype(str))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f5f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pool of proxies\n",
    "proxies = {v for v in data['URL']}\n",
    "\n",
    "\n",
    "url = 'https://www.google.com/'\n",
    "\n",
    "# Iterate the proxies and check if it is working.\n",
    "for proxy in proxies:\n",
    "    try:\n",
    "\n",
    "        page = requests.get(\n",
    "        url, proxies={\"http\": proxy, \"https\": proxy}, timeout=60)\n",
    "\n",
    "        # Prints Proxy server IP address if proxy is alive.\n",
    "        print(\"Status OK with {}, Output:\".format(proxy), page.status_code)\n",
    "\n",
    "    except OSError as e:\n",
    "\n",
    "        # Proxy returns Connection error\n",
    "        data = data[data.URL != proxy]\n",
    "        print(e)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3272da72",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('proxies_{}.csv'.format(date.today().strftime('%Y-%m-%d')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
